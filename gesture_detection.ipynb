{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEmfs0oWQptx",
        "outputId": "75f1ab41-af32-4168-a623-831b8b34dc8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "# Load desired gesture representation (image or video clip)\n",
        "desired_gesture = cv2.imread('/content/Screenshot 2024-04-09 212336.png')  # Update the path with your desired gesture image\n",
        "\n",
        "# Function to detect gesture in a frame\n",
        "def detect_gesture(frame, gesture_template):\n",
        "    # Convert frame and template to grayscale\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray_template = cv2.cvtColor(gesture_template, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Perform template matching\n",
        "    result = cv2.matchTemplate(gray_frame, gray_template, cv2.TM_CCOEFF_NORMED)\n",
        "\n",
        "    # Set threshold\n",
        "    threshold = 0.7  # Adjust threshold as needed\n",
        "\n",
        "    # Find locations where the correlation coefficient is above the threshold\n",
        "    loc = np.where(result >= threshold)\n",
        "\n",
        "    return loc, np.max(result)  # Return locations and maximum correlation coefficient\n",
        "\n",
        "# Function to annotate frame with \"DETECTED\" text, accuracy, detection time, and resource usage\n",
        "def annotate_frame(frame, accuracy, detection_time, cpu_usage, memory_usage):\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1\n",
        "    font_thickness = 2\n",
        "    text_color = (0, 255, 0)  # Green color\n",
        "    corner_position = (20, 50)  # Position of the text in the top-left corner\n",
        "\n",
        "    # Annotate the frame with \"DETECTED\" text, accuracy, detection time, and resource usage\n",
        "    text = f'DETECTED (Accuracy: {accuracy:.2f}, Time: {detection_time:.2f} secs, CPU: {cpu_usage:.2f}%, Memory: {memory_usage:.2f}%)'\n",
        "    cv2.putText(frame, text, corner_position, font, font_scale, text_color, font_thickness)\n",
        "\n",
        "# Function to draw bounding box around detected gesture\n",
        "def draw_bounding_box(frame, loc):\n",
        "    for pt in zip(*loc[::-1]):\n",
        "        cv2.rectangle(frame, pt, (pt[0] + desired_gesture.shape[1], pt[1] + desired_gesture.shape[0]), (0, 255, 0), 2)\n",
        "\n",
        "# Read test video\n",
        "test_video_path = '/content/WIN_20240409_20_55_43_Pro.mp4'  # Update the path with your test video\n",
        "cap = cv2.VideoCapture(test_video_path)\n",
        "\n",
        "# Get video properties\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Define codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "output_video = cv2.VideoWriter('/content/output_video3.avi', fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Initialize variables for timing\n",
        "start_time = time.time()\n",
        "previous_time = start_time\n",
        "\n",
        "# Loop through video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Detect gesture in the frame\n",
        "    gesture_loc, accuracy = detect_gesture(frame, desired_gesture)\n",
        "\n",
        "    # Calculate detection time\n",
        "    current_time = time.time()\n",
        "    detection_time = current_time - previous_time\n",
        "    previous_time = current_time\n",
        "\n",
        "    # Get CPU usage\n",
        "    cpu_usage = psutil.cpu_percent()\n",
        "\n",
        "    # Get memory usage\n",
        "    memory_usage = psutil.virtual_memory().percent\n",
        "\n",
        "    # If gesture is detected, annotate the frame\n",
        "    if gesture_loc[0].size > 0:\n",
        "        annotate_frame(frame, accuracy, detection_time, cpu_usage, memory_usage)\n",
        "        draw_bounding_box(frame, gesture_loc)\n",
        "\n",
        "    # Write annotated frame to output video\n",
        "    output_video.write(frame)\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "output_video.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "R6SdfCtoWCqk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5SCavlySbT4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}